# index_pdfs_advanced.py
from langchain_community.document_loaders import PyMuPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from pathlib import Path
import fitz  # PyMuPDF
from PIL import Image
import io
import os

# Camelot ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
try:
    import camelot

    CAMELOT_AVAILABLE = True
except ImportError:
    CAMELOT_AVAILABLE = False
    print("âš ï¸ Camelotì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ í‘œ ì¶”ì¶œ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("   ì •í™•í•œ í‘œ ì¶”ì¶œì„ ì›í•˜ì‹œë©´: pip install camelot-py[cv] opencv-python")


DOCS_DIR = "./docs"
DB_DIR = "./data/chroma_db"
IMAGE_DIR = "./data/extracted_images"
TABLE_DIR = "./data/extracted_tables"
EMB_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# dir_list = [DOCS_DIR, DB_DIR, IMAGE_DIR, TABLE_DIR]
#
# for i in dir_list:
#     if not os.path.exists(i):
#         os.makedirs(i)
#

# ì„¤ì •
MIN_IMAGE_SIZE = 10000  # 10KB ì´í•˜ ì´ë¯¸ì§€ëŠ” ë¬´ì‹œ (ì•„ì´ì½˜ ë“± ì œì™¸)
EXTRACT_TABLES = True  # í‘œ ì¶”ì¶œ í™œì„±í™”
EXTRACT_IMAGES = True  # ì´ë¯¸ì§€ ì¶”ì¶œ í™œì„±í™”


def extract_images_from_pdf(pdf_path):
    """PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥"""
    if not EXTRACT_IMAGES:
        return []

    doc = fitz.open(pdf_path)
    images_info = []

    Path(IMAGE_DIR).mkdir(exist_ok=True)
    pdf_name = Path(pdf_path).stem

    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images(full=True)

        for img_idx, img in enumerate(image_list):
            try:
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]

                # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ í•„í„°ë§ (ì•„ì´ì½˜, ë¡œê³  ë“±)
                if len(image_bytes) < MIN_IMAGE_SIZE:
                    continue

                # ì´ë¯¸ì§€ ì €ì¥
                image_filename = f"{pdf_name}_p{page_num + 1}_img{img_idx + 1}.{image_ext}"
                image_path = Path(IMAGE_DIR) / image_filename

                with open(image_path, "wb") as img_file:
                    img_file.write(image_bytes)

                # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ ì¶”ê°€
                try:
                    with Image.open(io.BytesIO(image_bytes)) as pil_img:
                        width, height = pil_img.size
                except:
                    width, height = None, None

                images_info.append({
                    "page": page_num + 1,
                    "path": str(image_path),
                    "filename": image_filename,
                    "size": len(image_bytes),
                    "dimensions": f"{width}x{height}" if width else "unknown"
                })
            except Exception as e:
                print(f"  âš ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ (í˜ì´ì§€ {page_num + 1}): {e}")

    doc.close()
    return images_info


def extract_tables_camelot(pdf_path):
    """Camelotìœ¼ë¡œ í‘œ ì •í™•í•˜ê²Œ ì¶”ì¶œ"""
    if not CAMELOT_AVAILABLE or not EXTRACT_TABLES:
        return []

    tables_data = []
    pdf_name = Path(pdf_path).stem
    Path(TABLE_DIR).mkdir(exist_ok=True)

    try:
        # lattice ëª¨ë“œ: ì„ ìœ¼ë¡œ êµ¬ë¶„ëœ í‘œ
        tables_lattice = camelot.read_pdf(str(pdf_path), pages='all', flavor='lattice')

        for i, table in enumerate(tables_lattice):
            # ì •í™•ë„ê°€ ë‚®ì€ í‘œëŠ” ì œì™¸
            if table.parsing_report['accuracy'] < 50:
                continue

            # CSVë¡œ ì €ì¥
            csv_filename = f"{pdf_name}_p{table.page}_table{i + 1}.csv"
            csv_path = Path(TABLE_DIR) / csv_filename
            table.to_csv(str(csv_path))

            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            markdown_table = table.df.to_markdown(index=False)

            tables_data.append({
                "page": table.page,
                "content": markdown_table,
                "csv_path": str(csv_path),
                "accuracy": table.parsing_report['accuracy'],
                "type": "lattice"
            })

        # stream ëª¨ë“œ: ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í‘œ
        try:
            tables_stream = camelot.read_pdf(str(pdf_path), pages='all', flavor='stream')
            for i, table in enumerate(tables_stream):
                if table.parsing_report['accuracy'] < 50:
                    continue

                csv_filename = f"{pdf_name}_p{table.page}_table_stream{i + 1}.csv"
                csv_path = Path(TABLE_DIR) / csv_filename
                table.to_csv(str(csv_path))

                markdown_table = table.df.to_markdown(index=False)

                tables_data.append({
                    "page": table.page,
                    "content": markdown_table,
                    "csv_path": str(csv_path),
                    "accuracy": table.parsing_report['accuracy'],
                    "type": "stream"
                })
        except:
            pass  # stream ëª¨ë“œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

    except Exception as e:
        print(f"  âš ï¸ Camelot í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    return tables_data


def extract_tables_basic(pdf_path):
    """ê¸°ë³¸ í‘œ ì¶”ì¶œ (Camelot ì—†ì„ ë•Œ)"""
    if not EXTRACT_TABLES:
        return []

    doc = fitz.open(pdf_path)
    tables_text = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text("text")

        # í‘œ í˜•ì‹ ê°ì§€
        lines = text.split('\n')
        table_candidate = []

        for line in lines:
            # íƒ­ ë˜ëŠ” ë‹¤ì¤‘ ê³µë°±ì´ ìˆê³  ìˆ«ìë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
            if ('\t' in line or '  ' in line) and any(char.isdigit() for char in line):
                table_candidate.append(line)
            elif table_candidate and len(table_candidate) >= 3:
                # 3ì¤„ ì´ìƒì´ë©´ í‘œë¡œ íŒë‹¨
                tables_text.append({
                    "page": page_num + 1,
                    "content": '\n'.join(table_candidate),
                    "type": "basic"
                })
                table_candidate = []
            else:
                table_candidate = []

        # ë§ˆì§€ë§‰ í…Œì´ë¸” ì²˜ë¦¬
        if table_candidate and len(table_candidate) >= 3:
            tables_text.append({
                "page": page_num + 1,
                "content": '\n'.join(table_candidate),
                "type": "basic"
            })

    doc.close()
    return tables_text


def load_documents_with_media(docs_dir):
    """PDF/TXT ë¡œë“œ + ì´ë¯¸ì§€/í‘œ ì¶”ì¶œ"""
    paths = list(Path(docs_dir).rglob("*.pdf")) + list(Path(docs_dir).rglob("*.txt"))
    docs = []
    total_images = 0
    total_tables = 0

    for p in paths:
        try:
            if p.suffix.lower() == ".pdf":
                print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {p.name}")

                # 1. ì´ë¯¸ì§€ ì¶”ì¶œ
                images = extract_images_from_pdf(str(p))
                if images:
                    print(f"  ğŸ“· {len(images)} ê°œì˜ ì´ë¯¸ì§€ ì¶”ì¶œë¨")
                    total_images += len(images)

                    for img_info in images:
                        img_doc = Document(
                            page_content=f"[ì´ë¯¸ì§€] {p.name} - í˜ì´ì§€ {img_info['page']}\n"
                                         f"íŒŒì¼ëª…: {img_info['filename']}\n"
                                         f"í¬ê¸°: {img_info['size'] // 1024}KB, í•´ìƒë„: {img_info['dimensions']}\n"
                                         f"ê²½ë¡œ: {img_info['path']}",
                            metadata={
                                "source": str(p),
                                "type": "image",
                                "page": img_info['page'],
                                "image_path": img_info['path'],
                                "filename": p.name
                            }
                        )
                        docs.append(img_doc)

                # 2. í‘œ ì¶”ì¶œ (Camelot ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ ëª¨ë“œ)
                if CAMELOT_AVAILABLE:
                    tables = extract_tables_camelot(str(p))
                else:
                    tables = extract_tables_basic(str(p))

                if tables:
                    print(f"  ğŸ“Š {len(tables)} ê°œì˜ í‘œ ì¶”ì¶œë¨")
                    total_tables += len(tables)

                    for table_info in tables:
                        table_content = f"[í‘œ] {p.name} - í˜ì´ì§€ {table_info['page']}\n\n{table_info['content']}"

                        metadata = {
                            "source": str(p),
                            "type": "table",
                            "page": table_info['page'],
                            "filename": p.name,
                            "extraction_method": table_info['type']
                        }

                        if 'csv_path' in table_info:
                            metadata['csv_path'] = table_info['csv_path']
                            metadata['accuracy'] = table_info['accuracy']

                        table_doc = Document(
                            page_content=table_content,
                            metadata=metadata
                        )
                        docs.append(table_doc)

                # 3. ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ê°€
                pdf_docs = PyMuPDFLoader(str(p)).load()
                docs.extend(pdf_docs)
                print(f"  âœ“ {len(pdf_docs)} í˜ì´ì§€ í…ìŠ¤íŠ¸ ë¡œë“œë¨")

            else:  # TXT íŒŒì¼
                txt_docs = TextLoader(str(p), encoding="utf-8").load()
                docs.extend(txt_docs)
                print(f"âœ“ ë¡œë“œë¨: {p.name}")

        except Exception as e:
            print(f"âœ— ì‹¤íŒ¨: {p.name} - {e}")

    print(f"\nğŸ“Š ì¶”ì¶œ ìš”ì•½:")
    print(f"  - ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
    print(f"  - ì´ í‘œ: {total_tables}ê°œ")
    print(f"  - ì´ ë¬¸ì„œ: {len(docs)}ê°œ")

    return docs


def main():
    print("=" * 60)
    print("ğŸš€ PDF ì¸ë±ì‹± ì‹œì‘ (ê³ ê¸‰ ëª¨ë“œ)")
    print("=" * 60)

    # ì„¤ì • ì¶œë ¥
    print(f"\nâš™ï¸ ì„¤ì •:")
    print(f"  - ì´ë¯¸ì§€ ì¶”ì¶œ: {'âœ“' if EXTRACT_IMAGES else 'âœ—'}")
    print(f"  - í‘œ ì¶”ì¶œ: {'âœ“' if EXTRACT_TABLES else 'âœ—'}")
    print(f"  - Camelot: {'âœ“ ì‚¬ìš©' if CAMELOT_AVAILABLE else 'âœ— ê¸°ë³¸ ëª¨ë“œ'}")
    print(f"  - ìµœì†Œ ì´ë¯¸ì§€ í¬ê¸°: {MIN_IMAGE_SIZE // 1024}KB")

    # ë¬¸ì„œ ë¡œë“œ
    docs = load_documents_with_media(DOCS_DIR)

    if not docs:
        print("\nâš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. docs/ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # í…ìŠ¤íŠ¸ ë¶„í• 
    print(f"\nâœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    chunks = splitter.split_documents(docs)
    print(f"  â†’ {len(chunks)} ê°œì˜ ì²­í¬ ìƒì„±ë¨")

    # ì„ë² ë”© & ë²¡í„°DB ìƒì„±
    print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMB_MODEL,
        model_kwargs={'device': 'cpu'},  # GPU ìˆìœ¼ë©´ 'cuda'ë¡œ ë³€ê²½
        encode_kwargs={'normalize_embeddings': True}
    )

    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=DB_DIR
    )

    print(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"  - ë²¡í„° DB: {DB_DIR}")
    print(f"  - ì´ë¯¸ì§€: {IMAGE_DIR}")
    if EXTRACT_TABLES and CAMELOT_AVAILABLE:
        print(f"  - í‘œ CSV: {TABLE_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()