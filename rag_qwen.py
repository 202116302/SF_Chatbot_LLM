# ask.py
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama  # LangChainì˜ Ollama ë˜í¼
from langchain.schema import SystemMessage, HumanMessage
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

DB_DIR   = "./chroma_db"
EMB_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
OLLAMA_MODEL = "qwen2.5:7b"  # âœ… ollamaì— pullëœ ëª¨ë¸ ì´ë¦„
TOP_K = 4                          # ê²€ìƒ‰ ì²­í¬ ê°œìˆ˜ (ë¹„ìš©/ì •í™•ë„ ë°¸ëŸ°ìŠ¤ í¬ì¸íŠ¸)

SYSTEM_PROMPT = """ë„ˆëŠ” ìŠ¤ë§ˆíŠ¸ë†ì—… êµìœ¡ì„ ì§€ì›í•˜ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ ì›ì¹™ì„ ì§€í‚¨ë‹¤.
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” ë‚´ìš©ë§Œ ë§í•œë‹¤. ì—†ìœ¼ë©´ "ìë£Œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•œë‹¤.
2. ìƒì¶” ì¬ë°° ë°©ë²•ì€ ë‹¨ê³„ë³„(íŒŒì¢…â†’ìœ¡ë¬˜â†’ì •ì‹â†’í™˜ê²½ê´€ë¦¬â†’ë³‘í•´ì¶©â†’ìˆ˜í™•)ë¡œ ì„¤ëª…í•œë‹¤.
3. ìˆ˜ì¹˜(ì˜¨ë„, ìŠµë„, ì°¨ê´‘, ê´€ìˆ˜, pF, kPa ë“±)ëŠ” ì›ë¬¸ê³¼ ë‹¤ë¥´ê²Œ ë°”ê¾¸ì§€ ì•ŠëŠ”ë‹¤.
4. ë‹µë³€ ëì— ê·¼ê±°ê°€ ëœ PDF íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë¥¼ ì ëŠ”ë‹¤.
5. ë§íˆ¬ëŠ” ê³µì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ í•œë‹¤.
"""

QA_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œ ì¡°ê°ë“¤ì´ì•¼:\n"
        "{context}\n\n"
        "ìœ„ì˜ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µí•´ì¤˜:\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    ),
)

def strip_think(text: str) -> str:
    # DeepSeek R1ì´ ë‚´ë†“ëŠ” <think> íƒœê·¸ ì œê±°(ê°€ë³ê²Œ ì²˜ë¦¬)
    import re
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

def build_retriever():
    embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)
    vectordb = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    return vectordb.as_retriever(search_kwargs={"k": TOP_K})

def build_llm():
    # ChatOllamaëŠ” ollama ì„œë²„(ê¸°ë³¸ http://localhost:11434)ì— ì ‘ì†í•©ë‹ˆë‹¤.
    # í•„ìš”í•œ ê²½ìš° temperature, num_ctx, num_predict ë“±ì„ ì¡°ì ˆí•˜ì„¸ìš”.
    return ChatOllama(
        model=OLLAMA_MODEL,
        temperature=0.2,
        # num_ctx=4096,   # ë¬¸ë§¥ì°½ ì—¬ìœ 
        # num_predict=512
    )

def format_context(docs):
    lines = []
    for d in docs:
        src = d.metadata.get("source")
        page = d.metadata.get("page")
        lines.append(f"[source: {src}, page: {page}] {d.page_content}")
    return "\n\n".join(lines[:TOP_K])

def answer(question: str):
    retriever = build_retriever()
    llm = build_llm()

    docs = retriever.get_relevant_documents(question)
    context = format_context(docs)

    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=QA_PROMPT.format(context=context, question=question)),
    ]
    raw = llm.invoke(messages).content
    text = strip_think(raw)

    print("ğŸ§  ë‹µë³€:\n", text, "\n")
    print("ğŸ“ ê·¼ê±°:")
    for d in docs:
        print("-", d.metadata.get("source"), "p.", d.metadata.get("page"))

if __name__ == "__main__":
    # ì˜ˆì‹œ ì§ˆë¬¸
    answer("í˜„ì¬ ëˆ„ì ê´‘ëŸ‰ì´ 8 mol mâ»Â² dâ»Â¹ë¡œ ë‚®ì€ ìƒíƒœì¸ë°, ìƒì¶”ì˜ ê´‘í¬í™”ì ê³¼ ê´‘ë³´ìƒì ì„ ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´ ë³´ê´‘ì´ í•„ìš”í•œê°€ìš”?")
